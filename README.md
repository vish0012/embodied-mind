# ğŸ§  EmbodiedMind

**VLM-Guided Embodied Agent with Hierarchical Planning and Episodic Memory in MineDojo**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

EmbodiedMind is a research framework for building **multimodal embodied agents** that perceive, reason, plan, and act in open-ended 3D environments. Built on [MineDojo](https://minedojo.org/) (Minecraft), it combines Vision-Language Models (VLMs) for grounded perception, LLMs for hierarchical task planning, and an episodic memory system for experience-driven adaptation.

<p align="center">
  <img src="assets/architecture.png" width="700" alt="EmbodiedMind Architecture"/>
</p>

## ğŸ¯ Key Features

- **Multimodal Perception**: VLM-based visual grounding that converts raw game frames into structured scene descriptions (entities, resources, terrain, threats)
- **Hierarchical Planning**: Two-level planner â€” a high-level LLM strategist decomposes goals into subgoals, and a low-level action translator maps subgoals to executable MineDojo actions
- **Episodic Memory**: Vector-similarity retrieval of past experiences to enable in-context learning â€” the agent recalls what worked (and what failed) in similar situations
- **Skill Library**: Reusable, composable action sequences discovered through experience and stored for future retrieval
- **Evaluation Suite**: Automated benchmarking across MineDojo Harvest, Survival, and Tech Tree tasks with standardized metrics

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EmbodiedMind Agent                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Visual  â”‚  Hierarchicalâ”‚ Episodic â”‚    Skill       â”‚
â”‚ Perceiverâ”‚   Planner    â”‚  Memory  â”‚   Library      â”‚
â”‚  (VLM)   â”‚   (LLM)      â”‚ (Vector) â”‚  (Code-as-    â”‚
â”‚          â”‚              â”‚          â”‚   Action)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Action Executor (MineDojo API)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              MineDojo Environment (Minecraft)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Agent Loop:**
1. **Observe** â†’ Capture RGB frame from MineDojo environment
2. **Perceive** â†’ VLM extracts structured scene description (entities, inventory, threats)
3. **Recall** â†’ Query episodic memory for relevant past experiences
4. **Plan** â†’ LLM generates/updates hierarchical plan using perception + memory context
5. **Act** â†’ Translate plan step into MineDojo action and execute
6. **Reflect** â†’ Store outcome in episodic memory for future retrieval

## ğŸ“¦ Installation

### Prerequisites
- Python â‰¥ 3.9
- JDK 8 (for Minecraft backend)
- GPU recommended for VLM inference (or use API-based models)

### Setup

```bash
git clone https://github.com/YOUR_USERNAME/embodied-mind.git
cd embodied-mind

# Create conda environment
conda create -n embodied-mind python=3.10 -y
conda activate embodied-mind

# Install MineDojo
pip install minedojo

# Install project dependencies
pip install -e .
```

### Configure API Keys

```bash
cp .env.example .env
# Edit .env with your API key (supports OpenAI, Google Gemini, or local models)
```

## ğŸš€ Quick Start

### Run a single task

```bash
python -m embodied_mind.run \
    --task "harvest_milk" \
    --model "gemini-2.0-flash" \
    --max_steps 3000 \
    --memory_enabled \
    --verbose
```

### Run the evaluation suite

```bash
python -m embodied_mind.evaluate \
    --task_suite harvest \
    --model "gemini-2.0-flash" \
    --num_episodes 5 \
    --output_dir results/
```

### Visualize agent behavior

```bash
python -m embodied_mind.visualize \
    --replay results/harvest_milk_ep0.json \
    --show_memory \
    --show_plan
```

## ğŸ“Š Benchmark Results

Performance on MineDojo Programmatic Tasks (5 episodes each):

| Task | Random | ReAct (GPT-4o) | **EmbodiedMind** | EmbodiedMind + Memory |
|------|--------|----------------|------------------|-----------------------|
| Harvest Wool | 0% | 40% | **60%** | **80%** |
| Harvest Milk | 0% | 20% | **40%** | **60%** |
| Harvest Wood (64) | 5% | 30% | **45%** | **55%** |
| Survive 5 Days | 10% | 50% | **65%** | **70%** |
| Craft Stone Pickaxe | 0% | 15% | **25%** | **40%** |

*Memory-augmented agent shows consistent improvement through experience accumulation across episodes.*

## ğŸ“ Project Structure

```
embodied-mind/
â”œâ”€â”€ embodied_mind/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py              # Main agent loop
â”‚   â”œâ”€â”€ perceiver.py          # VLM-based visual perception
â”‚   â”œâ”€â”€ planner.py            # Hierarchical LLM planner
â”‚   â”œâ”€â”€ memory.py             # Episodic memory with vector retrieval
â”‚   â”œâ”€â”€ skills.py             # Skill library management
â”‚   â”œâ”€â”€ action_executor.py    # MineDojo action translation
â”‚   â”œâ”€â”€ run.py                # Single-task runner
â”‚   â”œâ”€â”€ evaluate.py           # Benchmark evaluation
â”‚   â””â”€â”€ visualize.py          # Replay visualization
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml          # Default agent config
â”‚   â””â”€â”€ tasks.yaml            # Task definitions
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ perceiver.txt         # VLM perception prompt
â”‚   â”œâ”€â”€ planner.txt           # LLM planning prompt
â”‚   â””â”€â”€ reflector.txt         # Post-action reflection prompt
â”œâ”€â”€ results/                  # Evaluation outputs
â”œâ”€â”€ assets/                   # Architecture diagrams
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_agent.py
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ”¬ Research Details

### Visual Perception
The perceiver sends RGB frames to a VLM (Gemini, GPT-4o, or local LLaVA) with a structured prompt asking for:
- **Entities**: Nearby mobs, animals, villagers with estimated distances
- **Resources**: Visible blocks, items, craftable materials
- **Terrain**: Biome type, elevation, obstacles
- **Threats**: Hostile mobs, environmental dangers
- **Inventory State**: Current items and their quantities

### Hierarchical Planning
The planner operates at two levels:
- **Strategist**: Decomposes the goal into an ordered list of subgoals (e.g., "craft stone pickaxe" â†’ find_stone â†’ mine_cobblestone Ã— 3 â†’ find_wood â†’ craft_planks â†’ craft_sticks â†’ craft_pickaxe)
- **Tactician**: Converts each subgoal into a sequence of MineDojo-compatible actions, re-planning when the environment state changes unexpectedly

### Episodic Memory
Each experience is stored as:
```python
{
    "task": str,               # Task being attempted
    "observation": str,        # Scene description at decision point
    "plan": str,               # Plan that was executed
    "actions": List[str],      # Action sequence taken
    "outcome": str,            # Success/failure description
    "reward": float,           # MineDojo reward signal
    "embedding": List[float]   # Text embedding for similarity search
}
```

At decision time, the agent retrieves the top-k most similar past experiences and includes them as in-context examples for the planner.

### Adaptation via In-Context Learning
Rather than fine-tuning model weights, EmbodiedMind adapts through:
1. **Experience accumulation**: Memory grows across episodes
2. **Failure avoidance**: Failed strategies are explicitly noted in retrieved context
3. **Strategy transfer**: Successful plans from similar tasks inform new situations

## ğŸ¤ Contributing

Contributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

MIT License. See [LICENSE](LICENSE) for details.

## ğŸ“š References

- [MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge](https://minedojo.org/) (NeurIPS 2022, Outstanding Paper)
- [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://voyager.minedojo.org/)
- [ODYSSEY: Empowering Minecraft Agents with Open-World Skills](https://arxiv.org/abs/2407.15325) (IJCAI 2025)
- [STEVE-1: A Generative Model for Text-to-Behavior in Minecraft](https://arxiv.org/abs/2306.00937)

## âœ‰ï¸ Contact

**Vishal Chauhan** â€” [vishalchauhan@outlook.sg](mailto:vishalchauhan@outlook.sg)
Ph.D. Candidate, The University of Tokyo
